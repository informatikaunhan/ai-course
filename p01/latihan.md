# Latihan Pertemuan 01: Pengantar Kecerdasan Artifisial dan Agen Cerdas

**Mata Kuliah:** Kecerdasan Artifisial (AI401)  
**SKS:** 3 SKS (Teori)  
**Pertemuan:** 01  
**Topik:** Pengantar Kecerdasan Artifisial dan Agen Cerdas  
**Pengampu:** Anindito, S.Kom., S.S., S.H., M.TI., CHFI  

---

## Petunjuk Pengerjaan

1. Kerjakan semua soal secara mandiri
2. Waktu pengerjaan: 120 menit
3. Untuk soal pilihan ganda, pilih satu jawaban yang paling tepat
4. Untuk soal uraian, jawab dengan lengkap dan sistematis
5. Studi kasus memerlukan analisis mendalam dan penerapan konsep

---

## Bagian A: Soal Pilihan Ganda (20 Soal)

### Soal 1
Siapa yang mengusulkan metode pengujian untuk menentukan apakah mesin dapat menunjukkan kecerdasan setara manusia?

A. John McCarthy  
B. Marvin Minsky  
C. Alan Turing  
D. Herbert Simon  
E. Allen Newell

---

### Soal 2
Konferensi yang secara resmi melahirkan istilah "Artificial Intelligence" diadakan pada tahun:

A. 1943  
B. 1950  
C. 1956  
D. 1969  
E. 1997

---

### Soal 3
Pendekatan AI yang berfokus pada pembuatan sistem yang bertindak untuk mencapai hasil optimal disebut:

A. Thinking Humanly  
B. Thinking Rationally  
C. Acting Humanly  
D. Acting Rationally  
E. Learning Adaptively

---

### Soal 4
Manakah yang BUKAN merupakan kemampuan yang diperlukan untuk lulus Total Turing Test?

A. Natural Language Processing  
B. Knowledge Representation  
C. Computer Vision  
D. Quantum Computing  
E. Robotics

---

### Soal 5
AI Winter pertama (1974-1980) disebabkan oleh faktor berikut, KECUALI:

A. Keterbatasan hardware komputasi  
B. Kompleksitas kombinatorial  
C. Kritik dari Lighthill Report  
D. Kegagalan Large Language Models  
E. Ekspektasi yang tidak realistis

---

### Soal 6
Dalam konsep agen, komponen yang berfungsi untuk menerima input dari lingkungan adalah:

A. Aktuator  
B. Agent Function  
C. Sensor  
D. Performance Measure  
E. State

---

### Soal 7
Agen yang hanya menggunakan condition-action rules tanpa memiliki memori disebut:

A. Model-Based Reflex Agent  
B. Simple Reflex Agent  
C. Goal-Based Agent  
D. Utility-Based Agent  
E. Learning Agent

---

### Soal 8
Jenis agen manakah yang dapat menyimpan informasi tentang bagian lingkungan yang tidak terlihat saat ini?

A. Simple Reflex Agent  
B. Model-Based Reflex Agent  
C. Hanya Goal-Based Agent  
D. Hanya Utility-Based Agent  
E. Hanya Learning Agent

---

### Soal 9
Perbedaan utama antara Goal-Based Agent dan Utility-Based Agent adalah:

A. Goal-Based Agent tidak memiliki internal state  
B. Utility-Based Agent tidak dapat mencapai tujuan  
C. Utility-Based Agent dapat membandingkan beberapa alternatif berdasarkan tingkat kepuasan  
D. Goal-Based Agent dapat belajar dari pengalaman  
E. Utility-Based Agent tidak memerlukan sensor

---

### Soal 10
Komponen Learning Agent yang bertugas memberikan feedback berdasarkan standar kinerja adalah:

A. Performance Element  
B. Learning Element  
C. Critic  
D. Problem Generator  
E. Actuator

---

### Soal 11
Lingkungan dimana agen dapat melihat seluruh state pada setiap waktu disebut:

A. Deterministic  
B. Static  
C. Fully Observable  
D. Episodic  
E. Discrete

---

### Soal 12
Permainan Poker termasuk lingkungan dengan karakteristik:

A. Fully Observable  
B. Deterministic  
C. Partially Observable  
D. Static  
E. Single Agent

---

### Soal 13
Lingkungan dimana keputusan saat ini mempengaruhi semua keputusan di masa depan disebut:

A. Episodic  
B. Sequential  
C. Dynamic  
D. Stochastic  
E. Continuous

---

### Soal 14
Sistem klasifikasi gambar untuk mendeteksi tumor memiliki karakteristik lingkungan:

A. Sequential dan Dynamic  
B. Episodic dan Static  
C. Sequential dan Static  
D. Episodic dan Dynamic  
E. Continuous dan Multi-Agent

---

### Soal 15
Dalam kerangka PEAS, "Performance" merujuk pada:

A. Kecepatan pemrosesan sistem  
B. Kriteria untuk mengukur kesuksesan agen  
C. Jumlah sensor yang digunakan  
D. Kompleksitas lingkungan  
E. Kemampuan belajar agen

---

### Soal 16
Untuk automated taxi driver, manakah yang termasuk dalam komponen Environment pada PEAS?

A. Kemudi dan akselerator  
B. GPS dan speedometer  
C. Keselamatan dan waktu tempuh  
D. Jalan, lalu lintas, dan pejalan kaki  
E. Kamera dan LIDAR

---

### Soal 17
Deep Blue yang mengalahkan Kasparov pada tahun 1997 menggunakan pendekatan:

A. Thinking Humanly - mensimulasikan cara berpikir grandmaster  
B. Acting Humanly - meniru gaya bermain manusia  
C. Thinking Rationally - menggunakan penalaran logis murni  
D. Acting Rationally - mencari langkah optimal dengan brute-force search  
E. Learning Adaptively - belajar dari permainan sebelumnya

---

### Soal 18
Lingkungan permainan Backgammon yang melibatkan dadu termasuk kategori:

A. Deterministic  
B. Static  
C. Stochastic  
D. Episodic  
E. Single Agent

---

### Soal 19
Manakah pernyataan yang BENAR tentang Rational Agent?

A. Rational agent selalu mengetahui hasil aktual dari tindakannya  
B. Rational agent memaksimalkan hasil yang diharapkan berdasarkan informasi tersedia  
C. Rational agent identik dengan omniscient agent  
D. Rational agent hanya bekerja di lingkungan fully observable  
E. Rational agent tidak dapat menangani ketidakpastian

---

### Soal 20
Komponen Learning Agent yang berfungsi menyarankan tindakan eksplorasi untuk mengumpulkan pengalaman baru adalah:

A. Performance Element  
B. Learning Element  
C. Critic  
D. Problem Generator  
E. Utility Function

---

## Bagian B: Soal Uraian (15 Soal)

### Soal 1 ⭐
Jelaskan perbedaan antara pendekatan "Thinking Humanly" dan "Acting Rationally" dalam pengembangan sistem AI! Berikan satu contoh sistem untuk masing-masing pendekatan!

---

### Soal 2 ⭐
Sebutkan dan jelaskan empat komponen utama yang diperlukan untuk lulus Total Turing Test!

---

### Soal 3 ⭐
Apa yang dimaksud dengan Turing Test? Jelaskan prosedur pengujiannya dan sebutkan dua kritik terhadap metode ini!

---

### Soal 4 ⭐
Jelaskan mengapa AI Winter terjadi dan apa pelajaran yang dapat diambil dari periode tersebut!

---

### Soal 5 ⭐⭐
Gambarkan dan jelaskan diagram interaksi antara agen dengan lingkungannya! Identifikasi komponen sensor, aktuator, dan agent function!

---

### Soal 6 ⭐⭐
Jelaskan perbedaan antara Agent Function dan Agent Program! Mengapa tidak praktis untuk mengimplementasikan Agent Function secara langsung sebagai lookup table?

---

### Soal 7 ⭐⭐
Bandingkan Simple Reflex Agent dengan Model-Based Reflex Agent! Dalam situasi apa Model-Based Agent diperlukan? Berikan contoh!

---

### Soal 8 ⭐⭐
Jelaskan konsep Utility Function pada Utility-Based Agent! Mengapa pendekatan ini lebih baik daripada Goal-Based Agent untuk situasi dengan multiple conflicting goals?

---

### Soal 9 ⭐⭐
Sebutkan dan jelaskan empat komponen Learning Agent beserta fungsi masing-masing! Berikan contoh implementasi pada sistem rekomendasi film!

---

### Soal 10 ⭐⭐
Jelaskan enam dimensi karakteristik lingkungan tugas! Untuk setiap dimensi, berikan satu contoh lingkungan yang berada di masing-masing ujung spektrum!

---

### Soal 11 ⭐⭐
Apa perbedaan antara lingkungan Deterministic dan Stochastic? Berikan contoh dan jelaskan implikasinya terhadap desain agen!

---

### Soal 12 ⭐⭐⭐
Buat analisis PEAS lengkap untuk sistem robot bedah (surgical robot)! Jelaskan juga karakteristik lingkungannya berdasarkan enam dimensi!

---

### Soal 13 ⭐⭐⭐
Sebuah sistem trading saham otomatis harus memutuskan kapan membeli dan menjual saham. Analisis jenis agen minimum yang diperlukan dan jelaskan alasannya! Apakah Simple Reflex Agent cukup? Mengapa?

---

### Soal 14 ⭐⭐⭐
Jelaskan perbedaan antara lingkungan Episodic dan Sequential! Bagaimana perbedaan ini mempengaruhi kompleksitas pengambilan keputusan agen? Berikan contoh dari bidang pertahanan untuk masing-masing!

---

### Soal 15 ⭐⭐⭐
Desain utility function untuk drone pengantar paket yang harus menyeimbangkan: kecepatan pengiriman, keamanan (menghindari tabrakan), efisiensi baterai, dan kepuasan pelanggan. Tentukan bobot yang wajar dan berikan contoh perhitungan untuk dua alternatif rute!

---

## Bagian C: Studi Kasus (2 Kasus)

### Studi Kasus 1: Sistem Pertahanan Udara Terpadu

**Latar Belakang:**

TNI Angkatan Udara sedang mengembangkan sistem pertahanan udara terpadu yang mengintegrasikan beberapa komponen:
- 5 stasiun radar yang tersebar di wilayah perbatasan
- 3 baterai Surface-to-Air Missile (SAM)
- 2 skuadron pesawat tempur interceptor
- 1 Command and Control Center

Sistem harus mampu:
1. Mendeteksi dan melacak multiple airborne threats secara simultan
2. Mengklasifikasikan ancaman (pesawat sipil, pesawat militer asing, drone, rudal)
3. Memprioritaskan target berdasarkan tingkat bahaya
4. Mengalokasikan aset pertahanan (SAM atau interceptor) secara optimal
5. Belajar dari engagement sebelumnya untuk meningkatkan performa

**Pertanyaan:**

**1a.** Lakukan analisis PEAS lengkap untuk sistem ini! (15 poin)

**1b.** Analisis karakteristik lingkungan sistem ini berdasarkan enam dimensi! Jelaskan justifikasi untuk setiap dimensi! (15 poin)

**1c.** Tentukan jenis agen minimum yang diperlukan untuk sistem ini! Jelaskan mengapa jenis agen yang lebih sederhana tidak mencukupi! (10 poin)

**1d.** Rancang utility function untuk memprioritaskan target! Pertimbangkan faktor: military value target, jarak ke aset penting, waktu intercept, probabilitas keberhasilan, dan risiko collateral damage. Berikan contoh perhitungan untuk 2 skenario berbeda! (15 poin)

**1e.** Identifikasi komponen Learning Agent dalam sistem ini! Jelaskan bagaimana setiap komponen akan bekerja! (10 poin)

---

### Studi Kasus 2: Autonomous Underwater Vehicle (AUV) untuk Patroli Maritim

**Latar Belakang:**

TNI Angkatan Laut mengembangkan Autonomous Underwater Vehicle (AUV) untuk patroli dan surveillance di perairan Indonesia. AUV ini bertugas:
1. Melakukan patroli rutin di jalur pelayaran strategis
2. Mendeteksi kapal selam asing atau aktivitas mencurigakan
3. Mengumpulkan data oseanografi (temperatur, salinitas, arus)
4. Memetakan dasar laut dan mendeteksi ranjau atau objek mencurigakan
5. Berkomunikasi dengan command center ketika muncul ke permukaan

**Kendala Operasional:**
- Komunikasi bawah air sangat terbatas (acoustic modem dengan bandwidth rendah)
- Baterai terbatas dengan endurance 72 jam
- Harus menghindari deteksi oleh pihak asing
- Sensor sonar memiliki range terbatas dan rentan terhadap noise
- Kondisi laut (arus, temperatur layer) mempengaruhi performa sensor

**Pertanyaan:**

**2a.** Buat analisis PEAS lengkap untuk AUV ini! Identifikasi semua sensor dan aktuator yang diperlukan! (15 poin)

**2b.** Analisis karakteristik lingkungan berdasarkan enam dimensi! Berikan justifikasi detail untuk setiap dimensi berdasarkan kendala operasional yang disebutkan! (15 poin)

**2c.** Bandingkan kesesuaian tiga jenis agen untuk misi ini: Goal-Based Agent, Utility-Based Agent, dan Learning Agent! Tentukan mana yang paling sesuai dan jelaskan alasannya! (15 poin)

**2d.** Rancang condition-action rules untuk Simple Reflex Agent sebagai backup system ketika sistem utama gagal! Rules harus mencakup skenario: deteksi ancaman, baterai rendah, sensor malfunction, dan komunikasi darurat. (10 poin)

**2e.** Jelaskan bagaimana keterbatasan komunikasi bawah air mempengaruhi desain agen! Strategi apa yang dapat diterapkan untuk mengatasi kondisi partially observable yang ekstrem ini? (10 poin)

---

## Kunci Jawaban

### Bagian A: Pilihan Ganda

| No | Jawaban | Penjelasan |
|----|---------|------------|
| 1 | **C** | Alan Turing mengusulkan Turing Test pada tahun 1950 dalam paper "Computing Machinery and Intelligence" |
| 2 | **C** | Dartmouth Conference tahun 1956 adalah tempat lahirnya istilah "Artificial Intelligence" yang dicetuskan oleh John McCarthy |
| 3 | **D** | Acting Rationally adalah pendekatan yang berfokus pada tindakan optimal untuk mencapai hasil terbaik |
| 4 | **D** | Quantum Computing bukan bagian dari Total Turing Test. Komponen yang diperlukan: NLP, Knowledge Representation, Automated Reasoning, Machine Learning, Computer Vision, dan Robotics |
| 5 | **D** | Large Language Models baru muncul di era 2020-an, jauh setelah AI Winter pertama |
| 6 | **C** | Sensor berfungsi menerima input/percept dari lingkungan |
| 7 | **B** | Simple Reflex Agent hanya menggunakan condition-action rules tanpa memori |
| 8 | **B** | Model-Based Reflex Agent memiliki internal state untuk menyimpan informasi tentang bagian lingkungan yang tidak terlihat |
| 9 | **C** | Utility-Based Agent menggunakan utility function untuk mengukur "kebahagiaan" dan membandingkan alternatif |
| 10 | **C** | Critic bertugas mengevaluasi kinerja agen berdasarkan performance standard |
| 11 | **C** | Fully Observable berarti agen dapat melihat seluruh state lingkungan |
| 12 | **C** | Poker adalah Partially Observable karena kartu lawan tersembunyi |
| 13 | **B** | Sequential berarti keputusan saat ini mempengaruhi keputusan masa depan |
| 14 | **B** | Klasifikasi gambar tumor bersifat Episodic (setiap gambar independen) dan Static (gambar tidak berubah saat dianalisis) |
| 15 | **B** | Performance dalam PEAS adalah kriteria untuk mengukur kesuksesan agen |
| 16 | **D** | Environment mencakup objek di lingkungan: jalan, lalu lintas, pejalan kaki |
| 17 | **D** | Deep Blue menggunakan brute-force search untuk mencari langkah optimal, bukan meniru cara berpikir manusia |
| 18 | **C** | Backgammon bersifat Stochastic karena ada elemen dadu yang menambah ketidakpastian |
| 19 | **B** | Rational agent memaksimalkan hasil yang diharapkan berdasarkan informasi yang tersedia, bukan hasil aktual |
| 20 | **D** | Problem Generator menyarankan tindakan eksplorasi untuk mengumpulkan pengalaman baru |

---

### Bagian B: Uraian

#### Jawaban Soal 1 ⭐

**Thinking Humanly:**
- Fokus pada mensimulasikan proses kognitif manusia
- Menggunakan cognitive science dan neuroscience
- Tujuan: memahami dan mereplikasi cara berpikir manusia
- **Contoh:** Sistem yang mensimulasikan bias kognitif manusia dalam pengambilan keputusan (cognitive architecture seperti ACT-R)

**Acting Rationally:**
- Fokus pada tindakan yang mencapai hasil optimal
- Tidak perlu meniru proses berpikir manusia
- Menggunakan berbagai teknik termasuk yang tidak "mirip manusia"
- **Contoh:** AlphaGo yang menemukan strategi baru yang tidak pernah digunakan pemain manusia

**Perbedaan utama:** Thinking Humanly peduli pada "proses", Acting Rationally peduli pada "hasil".

---

#### Jawaban Soal 2 ⭐

Empat komponen untuk Total Turing Test:

1. **Natural Language Processing (NLP):** Kemampuan berkomunikasi dalam bahasa manusia, memahami dan menghasilkan teks

2. **Knowledge Representation:** Kemampuan menyimpan dan mengorganisasi pengetahuan tentang dunia

3. **Automated Reasoning:** Kemampuan menggunakan pengetahuan untuk menjawab pertanyaan dan menarik kesimpulan baru

4. **Machine Learning:** Kemampuan beradaptasi dan belajar dari pola baru

Untuk Total Turing Test, ditambahkan:

5. **Computer Vision:** Kemampuan mempersepsi objek visual

6. **Robotics:** Kemampuan memanipulasi objek fisik dan bergerak

---

#### Jawaban Soal 3 ⭐

**Turing Test:**
Metode pengujian yang diusulkan Alan Turing (1950) untuk menentukan apakah mesin dapat menunjukkan perilaku cerdas setara manusia.

**Prosedur:**
1. Penguji manusia berkomunikasi melalui teks dengan dua entitas (mesin dan manusia)
2. Penguji tidak mengetahui mana yang mesin
3. Penguji mengajukan pertanyaan bebas
4. Jika penguji tidak dapat membedakan mana yang mesin setelah waktu tertentu, mesin dianggap "lulus"

**Kritik:**
1. **Tidak perlu meniru manusia:** Pesawat tidak perlu mengepakkan sayap untuk bisa terbang. Kecerdasan tidak harus mirip manusia.
2. **Fokus pada penipuan:** Tes ini lebih mengukur kemampuan menipu daripada kecerdasan sebenarnya.
3. **Tidak menguji semua aspek kecerdasan:** Kreativitas, emosi, kesadaran tidak diuji.

---

#### Jawaban Soal 4 ⭐

**Penyebab AI Winter:**

1. **AI Winter 1 (1974-1980):**
   - Keterbatasan hardware (memori dan kecepatan)
   - Kompleksitas kombinatorial yang tidak terduga
   - Lighthill Report yang mengkritik kegagalan AI memenuhi janji
   - Pemotongan pendanaan riset

2. **AI Winter 2 (1987-1993):**
   - Expert systems sulit di-maintain dan tidak scalable
   - Biaya tinggi untuk hardware khusus AI
   - Keterbatasan expert systems dalam menangani ketidakpastian

**Pelajaran:**
1. Ekspektasi harus realistis dengan kapabilitas teknologi
2. Pentingnya riset fundamental, bukan hanya aplikasi
3. Kebutuhan akan pendekatan hybrid dan interdisipliner
4. Kemajuan hardware sangat penting untuk kemajuan AI

---

#### Jawaban Soal 5 ⭐⭐

**Diagram Interaksi Agen-Lingkungan:**

```
┌─────────────────────────────────────────┐
│              LINGKUNGAN                 │
└─────────────────┬───────────────────────┘
                  │
         Persepsi │ (melalui Sensor)
                  ▼
┌─────────────────────────────────────────┐
│                 AGEN                    │
│  ┌───────────────────────────────────┐  │
│  │         AGENT FUNCTION            │  │
│  │    f: Percept Sequence → Action   │  │
│  └───────────────────────────────────┘  │
└─────────────────┬───────────────────────┘
                  │
            Aksi  │ (melalui Aktuator)
                  ▼
┌─────────────────────────────────────────┐
│              LINGKUNGAN                 │
└─────────────────────────────────────────┘
```

**Komponen:**
- **Sensor:** Menerima input dari lingkungan (kamera, microphone, thermometer)
- **Agent Function:** Memetakan percept sequence ke action
- **Aktuator:** Melakukan tindakan ke lingkungan (motor, speaker, display)

---

#### Jawaban Soal 6 ⭐⭐

**Perbedaan:**

| Aspek | Agent Function | Agent Program |
|-------|----------------|---------------|
| Definisi | Deskripsi matematis abstrak | Implementasi konkret dalam kode |
| Representasi | Tabel mapping (bisa infinite) | Algoritma/program |
| Ukuran | Bisa sangat besar/infinite | Terbatas oleh memori |

**Mengapa lookup table tidak praktis:**

1. **Ukuran tabel eksponensial:** Untuk percept dengan n kemungkinan nilai selama t timesteps, ukuran tabel = n^t

2. **Contoh:** Robot dengan kamera 640×480 pixel, 24-bit color:
   - Satu frame: 640 × 480 × 2^24 kemungkinan
   - Percept sequence selama 1 jam: ukuran astronomis

3. **Tidak generalize:** Lookup table tidak dapat menangani situasi baru yang tidak ada di tabel

4. **Solusi:** Gunakan agent program yang mengkompresi agent function melalui algoritma cerdas

---

#### Jawaban Soal 7 ⭐⭐

**Perbandingan:**

| Aspek | Simple Reflex | Model-Based Reflex |
|-------|---------------|-------------------|
| Memori | Tidak ada (stateless) | Ada (internal state) |
| Keputusan berdasar | Percept saat ini saja | Percept + state |
| Cocok untuk | Fully observable | Partially observable |
| Kompleksitas | Rendah | Menengah |

**Kapan Model-Based diperlukan:**
- Lingkungan partially observable
- Perlu melacak objek yang tidak selalu terlihat
- Perlu mengingat aksi sebelumnya

**Contoh:** Robot vacuum dengan kamera terbatas
- Simple Reflex: Hanya bereaksi pada debu yang terlihat saat ini
- Model-Based: Mengingat peta ruangan dan area yang sudah dibersihkan, dapat kembali ke area yang terlewat meski tidak terlihat di kamera saat ini

---

#### Jawaban Soal 8 ⭐⭐

**Utility Function:**
Fungsi yang memetakan state ke bilangan real yang merepresentasikan tingkat kepuasan/kebahagiaan agen.

$$U: State \rightarrow \mathbb{R}$$

**Keunggulan dibanding Goal-Based:**

1. **Multiple conflicting goals:** Goal-Based hanya binary (tercapai/tidak). Utility dapat menyeimbangkan trade-off.
   
   *Contoh:* Self-driving car harus menyeimbangkan kecepatan vs keamanan vs kenyamanan

2. **Probabilistic outcomes:** Utility dapat menghitung expected utility ketika hasil tidak pasti.

   *Contoh:* Investasi dengan berbagai tingkat risiko dan return

3. **Partial goal achievement:** Utility mengukur "seberapa dekat" dengan optimal, bukan hanya sukses/gagal.

4. **Comparison:** Dapat membandingkan dua state yang sama-sama mencapai goal

---

#### Jawaban Soal 9 ⭐⭐

**Empat Komponen Learning Agent:**

| Komponen | Fungsi | Contoh di Rekomendasi Film |
|----------|--------|---------------------------|
| **Performance Element** | Memilih tindakan berdasarkan percept | Model rekomendasi yang memprediksi rating dan menghasilkan daftar film |
| **Learning Element** | Memperbaiki performance element | Training pipeline yang memperbarui model dari data watch history baru |
| **Critic** | Mengevaluasi kinerja berdasarkan standar | Sistem yang mengukur click-through rate, watch time, user ratings |
| **Problem Generator** | Menyarankan aksi eksplorasi | Algoritma yang sesekali merekomendasikan film di luar preferensi untuk mengumpulkan data baru |

**Cara kerja:**
1. User menonton film → Performance Element merekomendasikan film berikutnya
2. Critic mengamati apakah user menonton rekomendasi tersebut
3. Feedback dikirim ke Learning Element
4. Learning Element memperbarui model rekomendasi
5. Problem Generator sesekali menyarankan genre baru untuk eksplorasi preferensi user

---

#### Jawaban Soal 10 ⭐⭐

**Enam Dimensi Karakteristik Lingkungan:**

| Dimensi | Definisi | Contoh Ujung Spektrum |
|---------|----------|----------------------|
| **Observable** | Seberapa lengkap agen dapat melihat state | Fully: Catur | Partially: Poker |
| **Deterministic** | Apakah state berikutnya pasti | Deterministic: Catur | Stochastic: Backgammon |
| **Episodic** | Apakah episode independen | Episodic: Klasifikasi gambar | Sequential: Catur |
| **Static** | Apakah lingkungan berubah saat agen berpikir | Static: Crossword | Dynamic: Real-time game |
| **Discrete** | Apakah state/action terbatas | Discrete: Catur | Continuous: Robot navigasi |
| **Single/Multi Agent** | Berapa agen di lingkungan | Single: Sudoku solver | Multi: Sepak bola |

---

#### Jawaban Soal 11 ⭐⭐

**Perbedaan:**

| Aspek | Deterministic | Stochastic |
|-------|---------------|------------|
| Definisi | State berikutnya pasti ditentukan oleh state + action | Ada elemen ketidakpastian |
| Prediksi | 100% akurat | Probabilistik |
| Contoh | Catur | Backgammon (dadu) |
| Planning | Dapat merencanakan dengan pasti | Perlu contingency planning |

**Implikasi untuk desain agen:**

1. **Deterministic:** Agen dapat menggunakan search tree standar, planning deterministik

2. **Stochastic:** Agen perlu:
   - Penalaran probabilistik
   - Expected value calculation
   - Contingency planning
   - Robust decision making
   - Algoritma seperti Expectiminimax (akan dibahas Pertemuan 5)

---

#### Jawaban Soal 12 ⭐⭐⭐

**PEAS Robot Bedah:**

| Komponen | Spesifikasi |
|----------|-------------|
| **Performance** | Akurasi operasi, minimalisasi trauma jaringan, waktu operasi, keberhasilan prosedur, keselamatan pasien |
| **Environment** | Ruang operasi, organ pasien, tim medis, instrumen bedah, monitor vital sign |
| **Actuators** | Lengan robotik presisi, instrumen bedah (scalpel, forceps), suction, kamera endoskopi |
| **Sensors** | Kamera HD/3D, sensor force/torque, sensor posisi, input dari ahli bedah, monitor vital sign pasien |

**Karakteristik Lingkungan:**

| Dimensi | Nilai | Justifikasi |
|---------|-------|-------------|
| Observable | Partially | Tidak semua organ terlihat, area dalam tubuh terbatas |
| Deterministic | Stochastic | Variasi anatomi pasien, respons jaringan tidak pasti |
| Episodic | Sequential | Setiap langkah mempengaruhi langkah selanjutnya |
| Static | Dynamic | Kondisi pasien dapat berubah (bleeding, vital signs) |
| Discrete/Continuous | Continuous | Posisi dan gerakan dalam ruang 3D kontinu |
| Agents | Multi-Agent | Kerjasama dengan ahli bedah dan tim medis |

---

#### Jawaban Soal 13 ⭐⭐⭐

**Analisis Sistem Trading Otomatis:**

**Apakah Simple Reflex Agent cukup? TIDAK**

**Alasan:**

1. **Partially Observable:** Tidak semua informasi pasar tersedia (insider information, berita yang belum dipublikasi)

2. **Stochastic:** Pergerakan harga saham tidak deterministic

3. **Sequential:** Keputusan beli/jual mempengaruhi portfolio dan opsi masa depan

4. **Perlu tracking:** Harus mengingat posisi portfolio, harga beli, history transaksi

**Jenis Agen Minimum: Utility-Based Agent**

**Alasan:**
- Perlu **internal state** untuk tracking portfolio → Model-Based
- Perlu **goal** (profit maksimal) → Goal-Based
- Perlu **trade-off** antara return vs risk → Utility-Based
- Utility function: U = Expected Return - Risk Penalty

**Idealnya: Learning Agent**
- Dapat belajar dari pattern historis
- Beradaptasi dengan perubahan kondisi pasar

---

#### Jawaban Soal 14 ⭐⭐⭐

**Perbedaan Episodic vs Sequential:**

| Aspek | Episodic | Sequential |
|-------|----------|------------|
| Dependensi | Episode independen | Keputusan terhubung |
| Horizon | Satu langkah | Multi langkah |
| Kompleksitas | O(1) per keputusan | Perlu planning |
| State tracking | Minimal | Penting |

**Pengaruh pada kompleksitas:**

1. **Episodic:** Keputusan optimal dapat dihitung langsung tanpa mempertimbangkan masa depan. Kompleksitas rendah.

2. **Sequential:** Perlu mempertimbangkan konsekuensi jangka panjang. Kompleksitas eksponensial dengan horizon planning.

**Contoh Pertahanan:**

**Episodic:** Sistem klasifikasi citra satelit
- Setiap gambar dianalisis independen
- Klasifikasi gambar A tidak mempengaruhi klasifikasi gambar B

**Sequential:** Sistem Command and Control operasi militer
- Keputusan alokasi pasukan mempengaruhi opsi taktis masa depan
- Penggunaan amunisi/bahan bakar membatasi aksi selanjutnya
- Posisi strategis yang diambil menentukan keunggulan taktis

---

#### Jawaban Soal 15 ⭐⭐⭐

**Utility Function Drone Pengantar:**

$$U(route) = w_1 \cdot Speed + w_2 \cdot Safety + w_3 \cdot Battery + w_4 \cdot Satisfaction$$

**Normalisasi komponen (0-1):**
- Speed = 1 / (delivery_time_minutes / 60)
- Safety = (1 - collision_probability)
- Battery = remaining_battery_percentage
- Satisfaction = customer_rating_factor

**Bobot yang wajar:**
- w₁ = 0.25 (kecepatan)
- w₂ = 0.35 (keamanan - prioritas tertinggi)
- w₃ = 0.20 (efisiensi baterai)
- w₄ = 0.20 (kepuasan pelanggan)

**Contoh Perhitungan:**

| Komponen | Route A | Route B |
|----------|---------|---------|
| Delivery time | 15 menit | 10 menit |
| Collision prob | 5% | 15% |
| Battery remaining | 60% | 40% |
| Satisfaction factor | 0.8 | 0.9 |

**Route A:**
- Speed = 1/(15/60) = 4 → normalized: 0.67
- Safety = 1 - 0.05 = 0.95
- Battery = 0.60
- Satisfaction = 0.80

$$U(A) = 0.25(0.67) + 0.35(0.95) + 0.20(0.60) + 0.20(0.80)$$
$$U(A) = 0.168 + 0.333 + 0.120 + 0.160 = 0.781$$

**Route B:**
- Speed = 1/(10/60) = 6 → normalized: 1.0
- Safety = 1 - 0.15 = 0.85
- Battery = 0.40
- Satisfaction = 0.90

$$U(B) = 0.25(1.0) + 0.35(0.85) + 0.20(0.40) + 0.20(0.90)$$
$$U(B) = 0.250 + 0.298 + 0.080 + 0.180 = 0.808$$

**Kesimpulan:** Route B optimal (U = 0.808 > 0.781) meskipun lebih berisiko, karena keunggulan kecepatan dan kepuasan pelanggan mengkompensasi.

---

### Bagian C: Studi Kasus

#### Studi Kasus 1: Sistem Pertahanan Udara Terpadu

**1a. Analisis PEAS (15 poin)**

| Komponen | Spesifikasi |
|----------|-------------|
| **Performance** | Detection rate, false alarm rate, response time, successful intercept rate, friendly asset survival, minimalisasi collateral damage, resource efficiency |
| **Environment** | Airspace (3D), friendly aircraft, enemy aircraft/missiles, civilian aircraft, terrain, weather, electronic warfare environment, friendly ground assets, civilian areas |
| **Actuators** | SAM launcher control, interceptor tasking commands, radar mode control, IFF interrogation, alert broadcasts, communication channels, display systems |
| **Sensors** | 5 radar stations (surveillance + tracking), IFF transponders, ESM/ELINT sensors, communication receivers, weather sensors, satellite data links, intelligence feeds |

**1b. Karakteristik Lingkungan (15 poin)**

| Dimensi | Nilai | Justifikasi |
|---------|-------|-------------|
| **Observable** | **Partially Observable** | Radar tidak 100% coverage (terrain masking, stealth aircraft), niat musuh tidak diketahui, possible jamming |
| **Deterministic** | **Stochastic** | Pergerakan musuh unpredictable, probabilitas intercept tidak pasti, cuaca berubah, possible countermeasures |
| **Episodic** | **Sequential** | Alokasi aset mempengaruhi kemampuan menghadapi ancaman berikutnya, amunisi terbatas, posisi asset berubah |
| **Static** | **Dynamic** | Situasi berubah dalam hitungan detik, ancaman bergerak cepat, window of engagement terbatas |
| **Discrete/Continuous** | **Continuous** | Posisi dalam ruang 3D, kecepatan, heading semua kontinu |
| **Agents** | **Multi-Agent** | Koordinasi radar-SAM-interceptor (cooperative), adversarial dengan musuh (competitive) |

**1c. Jenis Agen Minimum (10 poin)**

**Jenis agen minimum: Learning Agent**

**Mengapa agen lebih sederhana tidak cukup:**

1. **Simple Reflex:** ❌ Tidak dapat melacak multiple targets, tidak ada memori
2. **Model-Based Reflex:** ❌ Tidak dapat memprioritaskan target atau merencanakan alokasi
3. **Goal-Based:** ❌ Tidak dapat menyeimbangkan multiple objectives (intercept vs resource conservation)
4. **Utility-Based:** ⚠️ Dapat memprioritaskan, tapi tidak dapat improve dari engagement sebelumnya

**Learning Agent diperlukan karena:**
- Requirement #5: "Belajar dari engagement sebelumnya"
- Musuh dapat mengubah taktik, sistem harus beradaptasi
- Pattern recognition untuk prediksi ancaman lebih baik

**1d. Utility Function untuk Prioritas Target (15 poin)**

$$U(target) = w_1 \cdot MilValue + w_2 \cdot Proximity + w_3 \cdot P_{success} - w_4 \cdot CollateralRisk - w_5 \cdot ResourceCost$$

**Komponen:**
- MilValue: Nilai militer target (0-100)
- Proximity: 1 - (distance_to_asset / max_distance)
- P_success: Probabilitas intercept berhasil
- CollateralRisk: Risiko kerusakan sipil (0-1)
- ResourceCost: Biaya resource normalized

**Bobot:** w₁=0.30, w₂=0.25, w₃=0.20, w₄=0.15, w₅=0.10

**Skenario 1: Pesawat bomber menuju pangkalan**
| Faktor | Nilai |
|--------|-------|
| MilValue | 90 |
| Proximity | 0.8 (dekat) |
| P_success | 0.75 |
| CollateralRisk | 0.1 |
| ResourceCost | 0.3 |

$$U_1 = 0.30(90/100) + 0.25(0.8) + 0.20(0.75) - 0.15(0.1) - 0.10(0.3)$$
$$U_1 = 0.27 + 0.20 + 0.15 - 0.015 - 0.03 = 0.575$$

**Skenario 2: Drone recon menuju kota**
| Faktor | Nilai |
|--------|-------|
| MilValue | 40 |
| Proximity | 0.6 |
| P_success | 0.9 |
| CollateralRisk | 0.5 |
| ResourceCost | 0.1 |

$$U_2 = 0.30(40/100) + 0.25(0.6) + 0.20(0.9) - 0.15(0.5) - 0.10(0.1)$$
$$U_2 = 0.12 + 0.15 + 0.18 - 0.075 - 0.01 = 0.365$$

**Kesimpulan:** Target 1 (bomber) diprioritaskan (U=0.575 > 0.365)

**1e. Komponen Learning Agent (10 poin)**

| Komponen | Implementasi |
|----------|--------------|
| **Performance Element** | Sistem yang melakukan deteksi, tracking, threat assessment, dan weapon assignment |
| **Learning Element** | Algoritma yang memperbarui model prediksi trajectory musuh, memperbaiki threat classification, dan mengoptimalkan weapon-target pairing dari data engagement |
| **Critic** | Mengevaluasi: intercept success rate, false alarm rate, response time, resource efficiency dibanding standar |
| **Problem Generator** | Menyarankan: simulasi skenario baru, war gaming untuk eksplorasi strategi, sensor mode alternatif untuk mengumpulkan data baru |

---

#### Studi Kasus 2: AUV untuk Patroli Maritim

**2a. Analisis PEAS (15 poin)**

| Komponen | Spesifikasi |
|----------|-------------|
| **Performance** | Area coverage rate, detection rate, stealth maintenance, mission duration, data quality, navigation accuracy, battery efficiency |
| **Environment** | Underwater terrain, water column (temperature layers, currents), marine life, foreign submarines, surface vessels, underwater obstacles, mines, communication nodes |
| **Actuators** | Propulsion motors (main + maneuvering), buoyancy control, rudder/fins, acoustic transmitter, emergency beacon, camera/sensor gimbal, data recorder |
| **Sensors** | Sonar (passive + active), hydrophone array, depth sensor, INS (Inertial Navigation System), Doppler velocity log, CTD sensor (conductivity, temperature, depth), magnetometer, camera, GPS (surface only) |

**2b. Karakteristik Lingkungan (15 poin)**

| Dimensi | Nilai | Justifikasi |
|---------|-------|-------------|
| **Observable** | **Highly Partially Observable** | Sonar range terbatas, thermocline menghalangi propagasi suara, tidak ada komunikasi real-time, GPS hanya di permukaan |
| **Deterministic** | **Stochastic** | Arus laut berubah, target bergerak unpredictable, sensor noise tinggi, kondisi akustik variabel |
| **Episodic** | **Sequential** | Konsumsi baterai kumulatif, posisi mempengaruhi opsi manuver, data yang dikumpulkan membentuk gambaran situasi |
| **Static** | **Dynamic** | Target bergerak, arus berubah, kondisi akustik berubah dengan waktu dan kedalaman |
| **Discrete/Continuous** | **Continuous** | Navigasi 3D (x, y, depth), heading, kecepatan semua kontinu |
| **Agents** | **Multi-Agent** | Possible encounter dengan kapal selam asing (adversarial), koordinasi dengan command center (cooperative, tapi terbatas) |

**2c. Perbandingan Jenis Agen (15 poin)**

| Aspek | Goal-Based | Utility-Based | Learning |
|-------|------------|---------------|----------|
| **Kelebihan** | Simple planning untuk patrol route | Dapat balance coverage vs stealth vs battery | Adaptif terhadap kondisi baru |
| **Kekurangan** | Tidak dapat trade-off | Tidak improve | Perlu banyak data, kompleks |
| **Cocok untuk** | Misi sederhana | Misi dengan constraints | Misi long-term |

**Pilihan optimal: Utility-Based Agent dengan elemen Learning**

**Alasan:**
1. **Trade-off kritis:** Misi memerlukan keseimbangan antara stealth (tidak terdeteksi), coverage (area patroli), dan battery (endurance 72 jam). Goal-Based tidak dapat menyeimbangkan ini.

2. **Keterbatasan komunikasi:** AUV harus autonomous dalam pengambilan keputusan. Utility function dapat di-program sebelum misi.

3. **Learning terlalu kompleks:** Komunikasi terbatas menyulitkan transfer learning data. Utility-Based cukup robust.

4. **Hybrid approach:** Utility function utama dengan komponen learning sederhana untuk adaptasi kondisi akustik lokal.

**2d. Condition-Action Rules untuk Backup System (10 poin)**

```
RULE 1 - Deteksi Ancaman:
IF sonar_contact = submarine_signature 
   AND distance < safe_threshold
THEN change_depth(evasive)
     reduce_speed(silent)
     log_contact_data

RULE 2 - Baterai Rendah:
IF battery_level < 20%
THEN abort_mission
     navigate_to_surface_point
     activate_beacon

RULE 3 - Sensor Malfunction:
IF primary_sonar = failure
THEN switch_to_passive_mode
     reduce_speed
     shallow_depth_for_possible_surface

RULE 4 - Komunikasi Darurat:
IF emergency_flag = true
THEN ascend_to_periscope_depth
     transmit_emergency_message
     await_instructions(timeout=10min)
     IF no_response THEN return_to_base

RULE 5 - Collision Avoidance:
IF obstacle_ahead AND distance < 50m
THEN stop_propulsion
     adjust_depth(+20m or -20m)
     resume_after_clear
```

**2e. Pengaruh Keterbatasan Komunikasi dan Strategi (10 poin)**

**Pengaruh keterbatasan komunikasi:**

1. **Highly autonomous:** AUV harus dapat membuat keputusan tanpa konfirmasi command center
2. **Delayed feedback:** Learning element tidak dapat menerima feedback real-time
3. **Pre-programmed policies:** Sebagian besar logika harus di-embed sebelum misi
4. **Information uncertainty:** Tidak dapat verify deteksi dengan command center

**Strategi mengatasi partially observable ekstrem:**

1. **Rich internal state:** Menyimpan detailed map of explored area, history of contacts, environmental conditions

2. **Probabilistic tracking:** Memaintain probability distribution untuk lokasi target yang terdeteksi lalu hilang

3. **Scheduled surface intervals:** Periodik naik untuk GPS fix dan burst communication

4. **Dead reckoning + INS:** Combine inertial navigation dengan occasional GPS correction

5. **Adaptive sensing:** Adjust sonar parameters berdasarkan kondisi akustik yang dipelajari

6. **Conservative policies:** Dalam ketidakpastian tinggi, prioritaskan stealth dan survival

7. **Data logging:** Simpan semua data untuk post-mission analysis dan learning offline

---

## Rubrik Penilaian

### Pilihan Ganda
- Setiap soal benar: 2 poin
- Total: 40 poin

### Uraian
- Soal ⭐: maksimal 5 poin
- Soal ⭐⭐: maksimal 8 poin  
- Soal ⭐⭐⭐: maksimal 12 poin
- Total: 125 poin

### Studi Kasus
- Studi Kasus 1: 65 poin
- Studi Kasus 2: 65 poin
- Total: 130 poin

### Total Keseluruhan: 295 poin

---

## License

This repository is licensed under the **Creative Commons Attribution 4.0 International (CC BY 4.0)**.

Commercial use is permitted, provided attribution is given to the author.

© 2026 Anindito
